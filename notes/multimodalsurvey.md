## [Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/abs/1705.09406)
Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency, 1 Aug 2017, Carnegie Mellon University, IEEE PAMI

TLDR; Survey paper for research in the field of Multimodal Machine Learning, deemed necessary for AI to better understand the world around us

### Definitions
* Modality: "refers to the way in which something happens or is experienced"
* Multimodal Research: "it includes multiple such modalities"
* Taxonomy: scheme of classification

### Key Points
* Necessary for AI to better understand the world around us
* Paper focuses on 3 modalities:
    1. Natural language: written or spoken
    2. Visual signals: images or videos
    3. Vocal signals: sounds and para-verbal information (prosody and vocal expressions)
* 5 core technical challenges: representation (heterogeneity of the data), translation, alignment, fusion, and co-learning
<p align="center">
<img src="https://github.com/gcunhase/PaperNotes/blob/master/notes/imgs/multimodal_scheme.png" width="400" alt="Multimodal Scheme">
</p>

### Applications: A Historical Perspective
<p align="center">
<img src="https://github.com/gcunhase/PaperNotes/blob/master/notes/imgs/multimodal_applications.png" width="400" alt="Multimodal Applications">
</p>

* The McGurk effect
    * [Hearing lips and seeing voices](https://www.nature.com/articles/264746a0): *"When human subjects heard the syllable /ba-ba/ while watching the lips of a person saying /ga-ga/, they perceived a third sound: /da-da/."*
    * Motivated researchers in the speech community to extend their approaches with visual information

### 1. Multimodal Representations
* Learning how to represent and summarize heterogeneous (multimodal) data

### 2. Translation
* Translation (mapping) from one modality to another, subjective relationships
* Example: there are many ways to describe an image, and no correct way

### 3. Alignment
* "Identify the direct relations between (sub)elements from two or more different modalities"
* Example: align the steps in a recipe to a cooking video

### 4. Fusion
* Fuse "information from two or more modalities to perform a prediction"
* Example: predicting spoken words in audio-visual speech recognition

### 5. Co-learning
* "Transfer knowledge between modalities, their representation, and their predictive models"
* Example: relevant with limited resources such as annotated data

### Notes / Questions

### Results

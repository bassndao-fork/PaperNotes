## [A GRU-Gated Attention Model for Neural Machine Translation](https://arxiv.org/abs/1704.08430)
Biao Zhang at al., Submitted on 27 Apr 2017

TLDR; Context vectors for different target words produced by Attention Networks are quite similar to one another and therefore are insufficient in discriminatively predicting target words. As a solution, the authors propose a novel GRU-gated attention model (GAtt) for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. Experiments on NIST Chinese-English translation tasks show that the proposed model achieves significant improvements over the vanilla attention-based NMT.

### Key Points
* Attention network: produces a context vector for each target word prediction. 
* What makes NMT outperform conventional statistical machine translation (SMT) is the attention mechanism [Bahdanau et al., 2014], an information bridge between the encoder and the decoder that produces context vectors by dynamically detecting relevant source words for predicting the next target word.
* Vanilla attention mechanism suffers from its inadequacy in distinguishing different translation predictions.
* Possible reason: context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. 
* Gatt: increase the degree of variance in context vectors by refining source representations according to the partial translation generated by the decoder.
* GAtt uses a gated recurrent unit (GRU) to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU.
* The GRU-combined information forms a new source annotation vector.
* In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors.

### Notes / Questions
* What does the attention model do that the GRU didn't already do?
* What I think the proposed model looks like:
<figure>
<p align="center">
<img src="https://github.com/gcunhase/PaperNotes/blob/master/notes/imgs/cGRUatt_blocks2.png" width="600" alt="cGRUatt">
<figcaption><p align="center">cGRUatt structure</p></figcaption>
</p>
</figure>
<!---
Diagram source: https://www.lucidchart.com/documents/edit/087437f7-a9e1-41e5-8786-aba8b587e37a
-->

### Results
* NIST Chinese-English
* Significant improvements
* Analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation.
* [Code](https://github.com/DeepLearnXMU/CAEncoder-NMT)
